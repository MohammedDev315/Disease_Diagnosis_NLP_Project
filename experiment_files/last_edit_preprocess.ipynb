{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook we will clean\\preprocess the text data\n",
    "\n",
    "**Importing the needed library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hs/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/hs/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/hs/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import necessary modules\n",
    "import ssl\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Import WordNetLemmatizer, and stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# import spcay framework\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from spacy.symbols import amod\n",
    "# Import Counter\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/hs/Documents/SDAIA_DS_T5/Project04-NLP/experiments/data/all_chapters.pdf\"\n",
    "ff = open( path ,\"rb\")\n",
    "pdfReader = PyPDF2.PdfFileReader(ff)\n",
    "print(pdfReader.numPages)\n",
    "total_page = pdfReader.numPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the  article value from list type to str type in order to tokenize it.\n",
    "\n",
    "# Function to convert  \n",
    "def listToString(s): \n",
    "    \n",
    "    # initialize an empty string\n",
    "    str1 = \" \" \n",
    "    \n",
    "    # return string  \n",
    "    return (str1.join(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \" \"\n",
    "\n",
    "for page_num  in np.arange(0 , total_page):\n",
    "    pageObj = pdfReader.getPage(page_num)\n",
    "    extract = pageObj.extractText().split(\"\\n\")\n",
    "    article += listToString(extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 6863), ('.', 5403), ('the', 5115), ('and', 3826), ('of', 2841), ('to', 2304), ('in', 1734), ('or', 1464), ('a', 1279), ('(', 1070)]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('patient', 961), ('may', 887), ('lung', 484), ('pulmonary', 442), ('respiratory', 417), ('disease', 400), ('cause', 390), ('treatment', 355), ('chest', 330), ('also', 280), ('infection', 261), ('symptom', 247), ('usually', 240), ('sign', 238), ('fluid', 231), ('diagnosis', 227), ('result', 226), ('pain', 220), ('pressure', 214), ('include', 207)]\n"
     ]
    }
   ],
   "source": [
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in stopwords.words('english')]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 20 most common tokens\n",
    "print(bow.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \" \".join(lemmatized)\n",
    "# string = doc\n",
    "# df = pd.DataFrame([string], columns=['string_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = doc\n",
    "df = pd.DataFrame([string], columns=['string_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['string_values'], dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['spacy_doc'] = list(nlp.pipe(df.string_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>string_values</th>\n",
       "      <th>spacy_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>respiratory disorder introduction respiratory ...</td>\n",
       "      <td>(respiratory, disorder, introduction, respirat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       string_values  \\\n",
       "0  respiratory disorder introduction respiratory ...   \n",
       "\n",
       "                                           spacy_doc  \n",
       "0  (respiratory, disorder, introduction, respirat...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_adj = [token.text.lower() for doc in df.spacy_doc for token in doc if token.pos_=='ADJ']\n",
    "doc_noun = [token.text.lower() for doc in df.spacy_doc for token in doc if token.pos_=='NOUN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('patient', 460),\n",
       " ('respiratory', 413),\n",
       " ('pulmonary', 399),\n",
       " ('pleural', 172),\n",
       " ('joint', 168),\n",
       " ('chronic', 148),\n",
       " ('severe', 138),\n",
       " ('normal', 125),\n",
       " ('alveolar', 108),\n",
       " ('special', 103)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(doc_adj).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('patient', 491),\n",
       " ('lung', 456),\n",
       " ('treatment', 355),\n",
       " ('disease', 345),\n",
       " ('chest', 293),\n",
       " ('infection', 261),\n",
       " ('diagnosis', 227),\n",
       " ('pain', 216),\n",
       " ('pressure', 213),\n",
       " ('blood', 203)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(doc_noun).most_common(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_list = list(set(doc_adj))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"patient\", \"may\", \"disease\", \"cause\", \"treatment\", \"also\", \"symptom\", \"usually\", \"sign\",\n",
    "                \"diagnosis\", \"result\", \"pain\", \"include\", \"pressure\", \"lung\", \"pulmonary\", \"respiratory\",\n",
    "                \"chest\", \"fluid\", \"complication\", \"change\", \"blood\", \"infection\", \"therapy\", \"prevent\",\n",
    "                \"acute\", \"care\", \"child\", \"level\", \"air\", \"use\", \"severe\", \"help\", \"used\", \"exercise\",\n",
    "                \"normal\", \"incidence\", \"pneumonia\",\"tissue\", \"show\", \"chronic\", \"failure\", \"cast\", \"increased\",\n",
    "                \"monitor\", \"hypoxemia\", \"produce\", \"edema\", \"increase\", \"space\", \"occurs\", \"cough\", \"alveolar\", \n",
    "                \"heart\", \"pathophysiology\", \"sputum\", \"provide\", \"decreased\", \"pneumothorax\", \"test\", \"special\",\n",
    "                \"tube\", \"condition\", \"common\", \"surgery\",\"secretion\", \"fibrosis\", \"disorder\", \"pa\", \"area\", \"form\",\n",
    "                \"cell\", \"skin\", \"drainage\", \"tb\", \"year\", \"commonly\", \"check\", \"teach\", \"rest\", \"watch\", \"encourage\", \n",
    "                \"underlying\", \"consideration\", \"et\", \"early\", \"hour\", \"family\", \"need\", \"effusion\", \"body\", \"drug\", \"support\", \n",
    "                \"rate\", \"syndrome\", \"requires\", \"inflammation\", \"abg\", \"side\", \"infant\", \"however\", \"upper\", \"cor\", \"pulmonale\",\n",
    "                 \"ventilator\", \"mechanical\", \"breath\", \"maintain\" , \"foot\", \"day\", \"bed\", \"parent\", \"especially\", \"fever\", \"culture\",\n",
    "                'system', 'within', 'factor', 'amount', 'death', 'movement', 'progress', 'volume', 'one', 'stage', 'report',\n",
    "                'avoid', 'respiration', 'trauma', 'occur', 'atelectasis', 'hand', 'includes', 'weight', 'tendon', 'hypertension', \n",
    "                'le', 'time', 'lead', 'damage', 'causing', 'require', 'activity', 'injury', 'risk', 'mm', 'measure', 'examination',\n",
    "                'nerve', 'stress', 'make', 'al', 'see', 'decrease', 'age', 'hg''case', 'month', 'coughing', 'develops', 'formation', \n",
    "                'without', 'site', 'every', 'reduce', 'relieve', 'effect','percussion', 'ordered', 'develop', 'affect', 'loss', 'flow',\n",
    "                'lesion', 'technique', 'exposure', 'gas', 'finding', 'procedure', 'begin', 'wall', 'immediately', 'type', 'response', \n",
    "                'position', 'needed', 'administer', 'control', 'ass', 'increasing', 'although', 'tell', 'output', 'give', 'analysis',\n",
    "                'history', 'often' ,'week', 'home', 'perform','function', 'typically', 'frequently', 'adult', 'indicate', 'administration',\n",
    "                'explain', 'using', 'suggest', 'called', 'center', 'head', 'people', 'resulting', 'including', 'period', 'feature'\n",
    "                   ]\n",
    "\n",
    "                #    \"involve\", \"way\", \"remains\", \"line\", \"skill\", \"lifestyle\", \"specified\", \"clothes\", \"wood\", \"slowly\", \"flowing\",\n",
    "                # \"established\", \"demonstrates\", \"confirming\", \"fact\", \"casting\", \"hacking\", \"foundation\", \"waiting\", \"administered\", \"supported\",\n",
    "                #  \"resting\", \"detecting\", \"frustration\", \"reducing\", \"researcher\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stopwords = adj_list+stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stpwrd = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " adj_list = 1452\n",
      " stop_words = 216\n",
      " marge adj_list & stop_words =  1668\n",
      " english stop words = 179\n"
     ]
    }
   ],
   "source": [
    "print(f' adj_list = {len(adj_list)}')\n",
    "print(f' stop_words = {len(stop_words)}')\n",
    "print(f' marge adj_list & stop_words =  {len(new_stopwords)}')\n",
    "print(f' english stop words = {len(stpwrd)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after marge all of the stop words = 1847\n"
     ]
    }
   ],
   "source": [
    "stpwrd.extend(new_stopwords)\n",
    "\n",
    "print(f'after marge all of the stop words = {len(stpwrd)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bone', 194), ('muscle', 184), ('ventilation', 162), ('oxygen', 157), ('airway', 144), ('breathing', 107), ('copd', 98), ('obstruction', 70), ('acidosis', 58), ('hg', 56), ('collapse', 56), ('case', 56), ('artery', 52), ('aspiration', 52), ('bronchiectasis', 52), ('hip', 52), ('deformity', 52), ('distress', 50), ('silicosis', 46), ('pulse', 45), ('reveals', 45), ('serum', 45), ('medication', 45), ('brace', 45), ('inspiration', 44), ('membrane', 44), ('instruct', 44), ('leg', 42), ('advise', 42), ('cwp', 42), ('traction', 42), ('place', 41), ('thoracic', 41), ('arthritis', 41), ('tension', 40), ('organism', 39), ('tract', 38), ('empyema', 38), ('promote', 38), ('croup', 38), ('dystrophy', 38), ('disk', 38), ('clubfoot', 38), ('surface', 37), ('must', 37), ('hypoxia', 37), ('rule', 37), ('remove', 36), ('embolism', 36), ('generally', 36), ('exertion', 36), ('calcium', 36), ('kyphosis', 36), ('two', 35), ('take', 35), ('sleep', 35), ('color', 35), ('degree', 35), ('abnormality', 35), ('around', 35), ('pattern', 34), ('attack', 34), ('neck', 34), ('lobe', 34), ('hemothorax', 34), ('prevention', 34), ('status', 34), ('keep', 34), ('ards', 34), ('phase', 34), ('osteoporosis', 34), ('spread', 33), ('carefully', 33), ('possibly', 33), ('diffuse', 33), ('example', 32), ('concentration', 32), ('infiltrates', 32), ('identify', 32), ('capacity', 32), ('known', 32), ('physiotherapy', 32), ('shock', 32), ('hypotension', 32), ('discharge', 32), ('splint', 32), ('bleeding', 32), ('caused', 32), ('anxiety', 32), ('sarcoidosis', 32), ('diet', 32), ('person', 32), ('knee', 32), ('structure', 31), ('page', 31), ('part', 31), ('shift', 31), ('cyanosis', 31), ('arrhythmia', 31), ('depending', 31)]\n"
     ]
    }
   ],
   "source": [
    "# Remove all stop words: no_stops\n",
    "no_stops01 = [t for t in lemmatized if t not in stpwrd ]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(no_stops01)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Topic Modeling: **LDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \" \".join(no_stops01)\n",
    "string = corpus\n",
    "df01 = pd.DataFrame([string], columns=['string_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "# sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zj/h4lkddcs4fl9m1km9f9qfffc0000gn/T/ipykernel_4019/3935130688.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcount_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstpwrd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdoc_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/SDAIA_DS_T5/Course_Material/NBM_Unsupervised_Gamma/curriculum/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;31m# TfidfVectorizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1310\u001b[0m                 \u001b[0;34m\"Iterable over raw text documents expected, string object received.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "# Create a CountVectorizer for parsing/counting words\n",
    "count_vectorizer = CountVectorizer(stop_words=stpwrd)\n",
    "\n",
    "doc_word = count_vectorizer.fit_transform(df01['string_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (1, 4211), indices imply (4211, 4211)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zj/h4lkddcs4fl9m1km9f9qfffc0000gn/T/ipykernel_4019/3677137060.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/SDAIA_DS_T5/Course_Material/NBM_Unsupervised_Gamma/curriculum/.venv/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    670\u001b[0m                 )\n\u001b[1;32m    671\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 mgr = ndarray_to_mgr(\n\u001b[0m\u001b[1;32m    673\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m                     \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/SDAIA_DS_T5/Course_Material/NBM_Unsupervised_Gamma/curriculum/.venv/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m     \u001b[0m_check_values_indices_shape_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"array\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/SDAIA_DS_T5/Course_Material/NBM_Unsupervised_Gamma/curriculum/.venv/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0mpassed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mimplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape of passed values is {passed}, indices imply {implied}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (1, 4211), indices imply (4211, 4211)"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(doc_word.toarray(), count_vectorizer.get_feature_names_out()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e20053a6a21f60b20031b0e753dd017cb749c39f38e8781debb23d87a774e1c7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
